%% INTRODUCTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

% TODO: 
% -Footnote on inclusion of Jacobian correction
% - Do we cite the Aaron Dinner paper on the automatic discovery of reaction coordinates anywhere?
% - Add sentence here or in Theory: These barriers produce a memory effect, whereby trajectories that enter the state from one side of the barrier behave qualitatively differently from ones that enter from the other side, until an adequate amount of time has passed for many barrier crossing events within this state.  

Many biomolecular processes are fundamentally dynamic in nature.
Protein folding, for example, involves the ordering of a polypeptide chain into a particular topology over the course of microseconds to seconds, a process which can go awry and lead to misfolding or aggregation, causing disease \cite{dobson:nature:2003:misfolding}.
Enzymatic catalysis may involve transitions between multiple conformational substates, only some of which may allow substrate access or catalysis \cite{kern:science:2002,youngblood:jbc:2006a,boehr:2006a}.
Post-translational modification events, ligand binding, or catalytic events may alter the transition kinetics among multiple conformational states by modulating catalytic function, allowing work to be performed, or transducing a signal through allosteric change \cite{frauenfelder:pnas:2001,changeux:2005a,maki:jbc:2006a}.
A purely static description of these processes is insufficient for mechanistic understanding --- the dynamical nature of these events must be accounted for as well.

Unfortunately, these processes may involve molecular timescales of microseconds or longer, placing them well outside the range of typical detailed atomistic simulations employing explicit models of solvent.
Many of these systems are very large, limiting the length of trajectories that can be generated by molecular dynamics simulation.
However, due to the presence of many energetic barriers on the order of the thermal energy, the uncertainty in initial microscopic conditions, and the stochasticity introduced into the system by the surrounding solvent in contact with a heat bath, any suitable description of conformational dynamics must \emph{by necessity} be statistical in nature.
This has motivated the development of stochastic kinetic models of macromolecular dynamics which might conceivably be constructed from short dynamics simulations, yet provide a useful and accurate statistical description of dynamical evolution over long times. 

Several approaches have been used to construct of these models.
\emph{Transition interface sampling} (TIS) \cite{moroni:2004b}, \emph{milestoning} \cite{faradjian:2004a}, and methods based on commitment probability distributions \cite{rhee:2005a,berezhkovskii:2005a} attempt to describe dynamics along a one dimensional reaction coordinate, but these approaches are valid only if an appropriate reaction coordinate can be identified such that relaxation transverse to this coordinate is fast compared to diffusion along it.
Discrete-state, continuous-time master equation models, characterized by a matrix of phenomenological rate constants describing the rate of interconversion between states \cite{vankampen}, can be constructed by identifying local potential energy minima as states and estimating interstate transition rates by transition state theory \cite{czerminski:1990a,kunz:1995a,ball:1998b,levy:2001a,mortenson:2001a,mortenson:2002a,evans:2004a}.
Unfortunately, the number of minima, and hence the number of states, grows exponentially with system size, making the procedure prohibitively expensive for larger proteins or systems containing explicit solvent molecules.
Others have suggested that stochastic models of dynamics can be constructed by expansion of the appropriate dynamical operator in a basis set \cite{shalloway:1996a,ulitsky:1998a,shen:2003a}, but this approach appears to be limited by the great difficulty of choosing rapidly-convergent basis sets for large molecules, a process that is not fundamentally different from identifying the slow degrees of freedom.

Instead, much work has focused on the construction of discrete- or continuous-time Markov models to describe dynamics among a small number of states which may each contain many minima within large regions of configuration space \cite{grubmueller:1994a,degroot:2001a,swope:2004b,singhal:2004a,levy:2005a,sorin:2005b,sriraman:2005a,schultheis:2005a,singhal:2005a,elmer:2005b,park:2006a}.
In these models, it is hoped that a separation of timescales between fast \emph{intrastate} motion and slow \emph{interstate} motion allows the statistical dynamics to be modeled by stochastic transitions among the discrete set of metastable conformational states governed by first-order kinetics.
Such a separation of timescales would be a natural consequence of the widely held belief that the nature of the energy landscape of biomacromolecules is hierarchical \cite{ansari:1985a,bai:1989a,becker:1997a,levy:2001a,levy:2002a}.
If the system reaches local equilibrium \emph{within} the state before attempting to exit, the probability of transitioning to any other state will be independent of all but the current state.
This allows the process to be modeled with either a discrete-time Markov chain (e.g.\ Ref.\ \cite{singhal:2004a}) or a continuous-time master equation model with coarse-grained time (e.g.\ Ref.\ \cite{sriraman:2005a}).
In either model, processes occurring on timescales faster than a coarse-graining time, determined by the time to reach equilibrium within each state, cannot be resolved.

Markov models embody a concise description of the various kinetic pathways and their relative likelihood, facilitating comparison with experimental data and providing a powerful tool for mechanistic insight.
Once the model is constructed and the timescale for Markovian behavior determined, it can be used to compute the stochastic temporal evolution of either a single macromolecule or a population of noninteracting macromolecules, allowing direct comparison of simulated and experimental observables for both single-molecule or ensemble kinetics experiments.
In addition, useful properties difficult to access experimentally, such as state lifetimes \cite{swope:2004a}, relaxation from experimentally inaccessible prepared states \cite{chodera:mms:2006}, mean first-passage times \cite{singhal:2004a}, the existence of hidden intermediates \cite{ozkan:2002a}, and $P_\mathrm{fold}$ values or transmission coefficients \cite{lenz:2004a}, can easily be obtained.
This allows for both a thorough understanding of mechanism and the generation of new, experimentally testable hypotheses.

To build such a model, it is necessary to decompose configuration space into an appropriate set of metastable states.
If the low-dimensional manifold containing all the slow degrees of freedom is known a priori, then this can be partitioned into free energy basins to define the states, such as by examination of the potential of mean force \cite{swope:2004b,sriraman:2005a,sorin:2005b,elmer:2005b,chodera:mms:2006}.
In the absence of this knowledge, others have turned to conformational clustering techniques to identify conformationally distinct regions which may also be kinetically distinct \cite{karpen:1993a,degroot:2001a,singhal:2004a,levy:2005a}.

Instead, we adopt a strategy first suggested for the discovery of metastable states in biomolecular systems by researchers at the \emph{Konrad-Zuse-Zentrum f\"{u}r Informationstechnik} \cite{schuette:1999a}.
The principal idea is this: If configuration space could be decomposed into a large number of small cells, the probability of transitioning between these cells in a fixed evolution time could be measured.
This probability is a measure of \emph{kinetic connectivity} among the cells, which allows the identification of aggregates of these cells that approximate true metastable states \cite{schuette:2002b}.
Unfortunately, the choice of how to divide configuration space into cells is not straightforward.
Suppose one is to consider the analysis of some fixed amount of simulation data. 
If configuration space is decomposed very finely, the boundaries between metastable states can in principle be well-approximated, but the estimated cell-to-cell transition probabilities will become statistically unreliable.
On the other hand, if configuration space is decomposed too coarsely, the transition probabilities may be well-determined, but the boundaries between metastable states cannot be clearly resolved, potentially disrupting or destroying the Markovian behavior of interstate dynamics.
An optimal choice would ultimately require knowledge of the metastable regions in order to determine the best decomposition of space into cells.

In this work, we propose an iterative procedure to determine both the choice of cells and their aggregates to approximate the desired metastable states.
% JDC: Do we want to say how this differs from previously proposed methods, such as sequential dihedral PCCA, or self-organizing box maps?
We use a conformational clustering method to carve configuration space into an initial crude set of cells (\emph{splitting}), and a Monte Carlo simulated annealing procedure to collect metastable collections of cells into states (\emph{lumping}).
This cycle is repeated, with the splitting procedure now applied individually to each state to generate a new set of cells, and the lumping procedure applied to the entire set of cells to redefine states until further application of this procedure leaves the approximations to metastable states unchanged.
This procedure allows state boundaries to be iteratively refined, as regions that mistakenly have been included in one state can be split off and regrouped with the proper state.
Throughout this process, we require that the cells never become so small that estimation of the relevent transition matrix elements is statistically unreliable.
Our proposed method is efficient, of $\mathcal{O}(N)$ complexity in the number of stored configurations, and can be easily parallelized.

This paper is organized as follows:
In Section \ref{section:theory}, we give an overview of the Markov chain model and its construction, elaborate on desirable properties of an algorithm to partition configuration space into states, and outline the principles underlying the algorithm we present here.
In Section \ref{section:methods}, we provide a detailed description of the automatic state decomposition algorithm and its implementation.
In Section \ref{section:applications}, we apply this algorithm to three model peptide systems in explicit solvent to assess its performance: alanine dipeptide, the 12-residue engineered trpzip2 hairpin, and the 21-residue F$_s$ helix-forming peptide.
Finally, in Section \ref{section:discussion}, we discuss the advantages and shortcomings of our algorithm, with the hope that future state decomposition algorithms can address the remaining challenges.
